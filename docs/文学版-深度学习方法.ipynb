{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1983c9b-8b20-4ef3-bdb1-3a54532d7710",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 深度学习方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e906cd-3f14-4203-a643-41a42c9c2adf",
   "metadata": {},
   "source": [
    "在时间的长河中，传统的时间序列模型如ARIMA犹如一位老者，经验丰富，解决问题得心应手。然而，这些传统方法也不乏局限性。传统的时间序列模型只是线性函数，或者线性函数的简单转换，需要人工诊断参数，如时间依赖性，并且在数据受损或缺失时表现不佳。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc5930-4ebd-4759-8a51-250d1d76239b",
   "metadata": {},
   "source": [
    "当我们探寻深度学习在时间序列预测领域的最新奥秘时，恍若发现了循环神经网络（RNN）近年来风靡一时。这些技术如同魔法般，能够娴熟地识别结构与模式，包括那些扭曲的非线性因素，毫不费力地处理着那些复杂的多变量输入问题，且对于缺失数据显得游刃有余。而RNN模型，则以其独特的方式，将状态从一次迭代传递至下一次，以自身的输出作为下一步的输入。这些深度学习“艺术家”也可被冠以时间序列模型的称号，因为它们能够借助过去的数据点，像是一位时光旅行者般，预见未来的趋势。就像传统的时间序列模型ARIMA一样，这些深度学习大师在金融领域有着广泛而深远的应用前景。接下来，让我们一同探索一下这些用于时间序列预测的深度学习“魔法师”们的神奇之处。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a76fed-31ab-47ea-acd4-7c914ab19a10",
   "metadata": {},
   "source": [
    "## 循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29195735-1650-4a46-b22d-e9fd56f5bac3",
   "metadata": {},
   "source": [
    "循环神经网络（RNNs）之所以被称为“循环”，是因为它们如同一位不知疲倦的行者，在序列中循环迈进，每一步都依赖于前方的足迹。RNN模型拥有一种记忆，它如同一本沉淀了过往智慧的书籍，记录着迄今为止所经历的一切。正如图5-4所描绘的，循环神经网络就像是一群并肩行进的同伴，共同传递着信息，传承着知识，每个人都在接力着前行的火炬。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d757fa-cfd3-41b2-ae64-bdee02c0c3f3",
   "metadata": {},
   "source": [
    "<img src='images/RNN.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1cb809-ba60-42a8-ba9f-2507009548fc",
   "metadata": {},
   "source": [
    "$X_t$代表时间步$t$的输入。$O_t$代表时间步t的输出。$S_t$代表时间步t的隐藏状态，它是网络的记忆。它根据前一个隐藏状态和当前步的输入计算得出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53643c19-2664-4144-883b-ce100d47b2a5",
   "metadata": {},
   "source": [
    "RNN的主要特征就是这个隐藏状态，它捕获了序列的一些信息，并在需要时相应地使用它。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c25ed7-bd6a-4712-8c76-8af568d9e66d",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592cce08-6136-446c-acee-f236c2bfd50a",
   "metadata": {},
   "source": [
    "长短期记忆（LSTM）就像一位专门训练的记忆达人，它能够避免忘记长时间以前的重要信息。在LSTM的世界里，每个“记忆细胞”都能够巧妙地捕获和储存数据流。这些“记忆细胞”之间还能够像过去的一座座桥梁，将信息从多个过去的时间点传递到当前时间点。而且，由于每个“记忆细胞”都搭载了“门”，所以它们可以对数据进行处理、筛选或者传递给下一个“记忆细胞”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499051d5-9932-46f9-9905-3c44535ff1c5",
   "metadata": {},
   "source": [
    "像是人工神经网络的“门”，让LSTM中的“记忆细胞”可以自由决定是否放行数据。每个门所产生的数字都在零到一之间，代表了每个细胞中各段数据应该通过的量。换言之，当估计值为零时，意味着“一点数据也不放过”。而估计值为一时，则象征着“让所有数据畅通无阻”。每个LSTM都有三种类型的门，旨在控制每个“记忆细胞”的状态："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97151301-290a-4cb3-8856-f7965e6861a4",
   "metadata": {},
   "source": [
    "**遗忘门**\n",
    "\n",
    "释放出介于零和一之间的数字，一代表“全心保留”，零则意味着“全然遗忘”。这个门会有条件地决定是否要遗忘或者保留过去的事情。\n",
    "\n",
    "**输入门**\n",
    "\n",
    "选择哪些新数据需要在“记忆细胞”中储存起来。\n",
    "\n",
    "**输出门**\n",
    "\n",
    "决定每个“记忆细胞”将会释放什么。这个释放出的值将会基于细胞的状态，以及经过筛选和新添加的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb755a7-dbf1-4c1b-b841-04c6f5464dd1",
   "metadata": {},
   "source": [
    "Keras就像是一位神奇的魔术师，它把高效的数值计算库和函数装进了一个小小的魔法盒子，让我们可以用几行短短的代码，就能定义和训练LSTM神经网络模型。在下面的代码中，我们使用了来自keras.layers的LSTM模块，搭建了一座由50个LSTM“魔法师”或神经元组成的隐层，以及一个输出层，用于做出单值预测。关于所有术语（例如模型序列、学习率、动量、轮次和批次样本量）的更详细描述，请参阅深度学习概念章节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c311b-1aa2-4e9d-b49d-155d2ef38146",
   "metadata": {},
   "source": [
    "下面是一个在Keras中实现LSTM模型的Python示例代码："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5805cf2-4b17-42d2-899b-31e7a5db6308",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "from keras.models import Sequential from keras.layers import Dense\n",
    "from keras.optimizers import SGD from keras.layers import LSTM\n",
    "\n",
    "def create_LSTMmodel(learn_rate = 0.01, momentum=0): \n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X_train_LSTM.shape[1],\\\n",
    "                                    X_train_LSTM.shape[2])))\n",
    "    \n",
    "    #More number of cells can be added if needed model.add(Dense(1))\n",
    "    optimizer = SGD(lr=learn_rate, momentum=momentum) \n",
    "    model.compile(loss='mse',optimizer='adam') \n",
    "    return model\n",
    "\n",
    "LSTMModel = create_LSTMmodel(learn_rate = 0.01, momentum=0)\n",
    "LSTMModel_fit = LSTMModel.fit(X_train_LSTM, Y_train_LSTM,\\\n",
    "                              validation_data= (X_test_LSTM, Y_test_LSTM),\\\n",
    "                              epochs=330, batch_size=72, verbose=0,\\\n",
    "                              shuffle=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeac9b6-dc22-4fe2-acd9-89daeaaa4d57",
   "metadata": {},
   "source": [
    "无论是在学习还是实践中，LSTM都为微调提供了更多的可能性，相较于ARIMA模型而言。虽然深度学习模型在许多方面都超越了传统的时间序列模型，但它们更为复杂，训练难度更大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe746408-dd0c-4597-aba9-7c3f4dad8f8e",
   "metadata": {},
   "source": [
    "## 将时间序列数据变形为监督学习模型\n",
    "\n",
    "时间序列就像时间的倒影，按照时间的先后顺序排列着一连串的数字。而在监督学习中，我们有输入变量（X）和输出变量（Y）。对于一个时间序列数据集的数字序列，我们可以将其重新构造成一组预测变量和目标变量，就像解决监督学习问题一样。我们可以通过将之前的时间步作为输入变量，将下一个时间步作为输出变量来实现这一点。让我们通过一个例子来说明这个过程。\n",
    "\n",
    "我们可以将图5-5左侧的时间序列重新构造为一个监督学习问题，即使用前一个时间步的值来预测下一个时间步的值。一旦我们按照这种方式重新组织了时间序列数据集，数据看起来就像图右侧的表格所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c6060-6823-4378-92d3-f9c615d779a8",
   "metadata": {},
   "source": [
    "<img src='images/timeseries_supervize.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e6562-a4db-4d26-8418-950d13f804ec",
   "metadata": {},
   "source": [
    "在我们的监督学习冒险中，前一个时间步是输入（X），而下一个时间步是输出（Y）。观察到的时序之间的秩序被保留了下来，并且在使用这个数据集来培训监督模型时，这一点必须继续保持。在培训我们的监督模型时，我们将删除第一行和最后一行，因为我们既没有X的值，也没有Y的值。\n",
    "\n",
    "在Python中，Pandas库的shift()函数是帮助将时间序列数据转换为监督学习问题的主要法宝。我们将在案例研究中展示这种方法。利用之前的时间步来预测下一个时间步的方法被称为滑动窗口、时间延迟或滞后方法。\n",
    "\n",
    "在探讨了监督学习和时间序列模型的所有概念之后，让我们一起踏入案例研究的领域吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff3be9-c698-45d4-aaac-f85d567968fc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
