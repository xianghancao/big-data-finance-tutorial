{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af9ce21-c654-425c-9db9-8c7725cdf020",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 线性回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df15a68-9f8d-421c-ad28-fc11b65180f5",
   "metadata": {},
   "source": [
    "## 1. 相关系数\n",
    "### 1.1 两个变量之间的相关系数\n",
    "我们先考虑下两个连续变量之间的统计关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ab8ed-2310-4954-87fe-cf359b7dd655",
   "metadata": {},
   "source": [
    "![simple_regression](images/simple_regression.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f84533-0b76-433a-be82-df8908398b12",
   "metadata": {
    "tags": []
   },
   "source": [
    "在线性回归分析开始前，一般计算自变量和因变量的相关系数，我们把这一步称之为相关分析。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726502c-d3c6-4521-abba-416ac84d7f0f",
   "metadata": {},
   "source": [
    "相关系数r检验y和x两个变量之间的线性相关的显著程度，其算式为\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum{(x_i-\\bar{x})^2\\sum{(y_i-\\bar{y})^2}}}}\n",
    "$$\n",
    "\n",
    "数学上可以证明：r在[-1, 1]范围，有：\n",
    "- r>0时，y与x有一定的正线性相关，越接近1正的相关性越大\n",
    "- r<0时，y与x有一定的负线性相关，越接近-1负的相关性越大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f33d8-d679-4b56-b8c0-fbe22120554d",
   "metadata": {},
   "source": [
    "### 1.2 相关性系数的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4841b51-1750-469a-a862-607b244f60f3",
   "metadata": {},
   "source": [
    "调用scipy.stats.pearsonr计算相关系数：\n",
    "```python\n",
    "import scipy.stats as stats\n",
    "r = stats.pearsonr(x1, x2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a520e51e-bf37-480e-9223-698b1f176e7d",
   "metadata": {},
   "source": [
    "也可以使用`pd.DataFrame.corr()`来计算相关系数矩阵，然后绘制图形，详细见下一节。\n",
    "```python\n",
    "df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)],\n",
    "                                columns=['dogs', 'cats'])\n",
    "df.corr(method='pearson')\n",
    "      dogs  cats\n",
    "dogs   1.0   0.3\n",
    "cats   0.3   1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b90177-a6e2-4a4d-9246-71b2406dd6b0",
   "metadata": {},
   "source": [
    "## 2. 一元线性回归的理论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b78fb9-3ce7-4f5b-81d3-e9d70a44feaa",
   "metadata": {},
   "source": [
    "**输出变量Y**被称为被解释变量、因变量、结果，而**输入变量X**可以被称为解释变量、自变量、效果、预测因子。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba9a0d-f677-4ca5-b66b-1b54337c4359",
   "metadata": {},
   "source": [
    "$$\n",
    "Y_i = \\beta_0+\\beta_1X_i+u_i \n",
    "$$\n",
    "\n",
    "$$\n",
    "i是第i次观测，i=1,2,...,n;Y_i是被解释变量，\\beta_0是截距；\\beta_1是总体回归线的斜率，u_i是误差项\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559987f1-1adc-40cd-8d9e-cca5ac14f3c5",
   "metadata": {},
   "source": [
    "线性回归拟合一个具有系数的线性模型，以最小化数据集内观测目标与线性逼近预测目标之间的残差平方和。数学上，它解决了这样一个问题:\n",
    "\n",
    "$$\n",
    "min\\{\\sum^{n}_{i=1}(Y-\\beta_0-\\beta_1X_i)^2\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f60222-0a01-4f54-ad3b-de9f34a4f4c4",
   "metadata": {},
   "source": [
    "### 2.1 OLS方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe571ed-3eb3-4687-8a28-cc90fe26623a",
   "metadata": {},
   "source": [
    "![OLS](images/OLS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449c32e5-486c-405f-b3a7-3da722329954",
   "metadata": {},
   "source": [
    "为了最小化预测误差平方和$\\sum^{n}_{i=1}(Y-\\beta_0-\\beta_1X_i)^2$，首先将该式关于$b_0$和$b_1$求偏导数，可以得到以下两个等式：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\sum(Y_i-\\beta_0-\\beta_1X_i)^2}{\\partial\\beta_0}\n",
    "= -2\\sum(Y_i-\\beta_0-\\beta_1X_i)^2\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\sum(Y_i-\\beta_0-\\beta_1X_i)^2}{\\partial\\beta_1}\n",
    "= -2\\sum(Y_i-\\beta_0-\\beta_1X_i)X_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcf5543-769d-4463-add1-c775e907e806",
   "metadata": {},
   "source": [
    "令上面2个偏导数等于零，整理后得到OLS估计量$\\beta_0$和$\\beta_1$必须满足的两个方程：\n",
    "\n",
    "$$\n",
    "\\bar{Y}-\\hat{\\beta_0}-\\hat{\\beta_1}\\bar{X}=0\n",
    "$$\n",
    "$$\n",
    "\\frac{1}{n}\\sum{X_i}{Y_i} - \\hat{\\beta_0}\\bar{X}-\\hat{\\beta_1}\\frac{1}{n}\\sum^n_{i=1}X^2_i = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d255dd-4a73-4867-8dbf-bb763531e50f",
   "metadata": {},
   "source": [
    "解上述关于$\\hat{\\beta_0}$和$\\hat{\\beta_1}$的方程组，得到\n",
    "\n",
    "$$\n",
    "\\hat{\\beta_1}=\\frac{\\frac{1}{n}\\sum_{i=1}^n X_i Y_i-\\bar{XY}}{\\frac{1}{n}\\sum_{i=1}^n X_i^2 - \\bar{X}^2}\n",
    "$$\n",
    "$$\n",
    "\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88880618-3dec-4f68-bfa6-f5285500ae11",
   "metadata": {},
   "source": [
    "## 3. 多元线性回归的理论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ccaac-6557-4291-a7cd-35b768a0bb8a",
   "metadata": {},
   "source": [
    "![multi_regression](images/multi_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb9edc-50c8-4b2c-8a0a-a837dc670431",
   "metadata": {},
   "source": [
    "多元回归模型是：\n",
    "\n",
    "$$\n",
    "Y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + ... +\\beta_kX_{ki} + \\mu_i,i=1,...,n\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $Y_i$是被解释变量的第$i$个观测值；$X_{1i},X_{2i},...,X_{ki}$是$k$个解释变量的第$i$个观测值；$\\mu_i$是误差项。\n",
    "- 总体回归线表示的是$Y$和$X$之间的总体平均关系。\n",
    "- $\\beta_1$是$X_1$的斜率系数；$\\beta2$是$X_2$的斜率系数，等等。\n",
    "- 截距$\\beta_0$是当所有解释变量$X$取值为零时$Y$期望值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56364176-1fb2-4852-9597-2ccb4179e357",
   "metadata": {},
   "source": [
    "\n",
    "估计量$\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_k}$为使得预测误差平方和$\\sum^n_{i=1}(Y_i-\\beta_0-\\beta_1X_{1i}-...-\\beta_kX_{ki})^2$达到最小的$\\beta_0,\\beta_1,...,\\beta_k$取值。\n",
    "\n",
    "预测值$\\hat{Y_i}$和残差$\\hat{u_i}$分别为：\n",
    "\n",
    "$$\n",
    "\\hat{Y_i}=\\hat{\\beta_0} + \\hat{\\beta_1}X_{1i} + ... + \\hat{\\beta_k}X_{ki}, i=1,...,n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{u_i}=Y_i-\\hat{Y_i}, i=1,...,n\n",
    "$$\n",
    "\n",
    "其中估计量$\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_k}$和残差$\\hat{u_i}$都是利用$n$组样本观测数据$(X_{1i}, ..., X_{ki},Y_i), i=1,...n$计算得到的。它们分别是未知真实总体系数$\\beta_0,\\beta_1,...,\\beta_k$和误差项$\\mu_i$的估计量。\n",
    "\n",
    "我们使用OLS方法求的$\\hat{\\beta_0},\\hat{\\beta_1},...,\\hat{\\beta_k}$，为普通最小二乘(OLS)估计量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f1a6e-6790-4933-afc0-961890034653",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 模型评价指标\n",
    "\n",
    "### 4.1 SER\n",
    "\n",
    "回归标准误（SER）是对误差项$\\mu_i$标准差的估计，故SER可用来度量$Y$的分布在回归线周围的离散程度。\n",
    "\n",
    "$$\n",
    "SER = s_{\\hat{u}}=\\sqrt{s^2_{\\hat{u}}}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866285f-983f-460f-a035-920b5deadac4",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 $R^2$\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{ESS}{TSS}=\\frac{\\sum^n_{i=1}{(\\hat{Y_i}-\\bar{Y})^2}}{\\sum^n_{i=1}{(Y_i-\\bar{Y})^2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{ESS}{TSS}=1-\\frac{SSR}{TSS}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R^2 = 1-\\frac{SSR}{TSS}\n",
    "$$\n",
    "\n",
    "$$\n",
    "SSR为残差平方和\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5474d-daa7-48b7-85f6-d5d73709f2df",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 调整$R^2$\n",
    "\n",
    "$$\n",
    "\\bar{R}^2=1-\\frac{n-1}{n-k-1}\\frac{SSR}{TSS}=1-\\frac{s^2_\\hat{u}}{s^2_Y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "k:斜率系数和截距的数量\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ccd106-75be-45f8-a3a6-24c1c234c508",
   "metadata": {},
   "source": [
    "## 5. 线性回归的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c4865-260b-4944-b5d8-98afd3b848d9",
   "metadata": {},
   "source": [
    "线性回归的Python实现主要使用sklearn机器学习库linear_model模块下的LinearRegression类。\n",
    "```python\n",
    "class sklearn.linear_model.LinearRegression(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)[source]¶\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd561e-afa4-4206-867b-16d2b3d49db4",
   "metadata": {},
   "source": [
    "|类别|名称|含义|\n",
    "|---|---|---|\n",
    "|**参数**|**fit_intercept**|默认为True，是否计算模型的截距，如果设为False，是指数据是以原点为中心的，不会计算截距。|\n",
    "|**属性**|**coef_**|线性回归问题的估计系数。|\n",
    "||**intercept_**|线性模型中的独立项，也就是截距|\n",
    "|**方法**|**fit(X, y)**|训练模型（或称估计器、学习器）|\n",
    "||**predict(X)**|在训练后，使用模型预测|\n",
    "||**score(X, y)**|用来计算模型的精度|\n",
    "||**get_params()**|获得模型的参数|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d6af7-78bd-4d7b-8662-6323fa85d36d",
   "metadata": {},
   "source": [
    "### 5.1 模型输入X和y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac338a8-e93a-4a4a-93c3-10c6872269ab",
   "metadata": {},
   "source": [
    "上述回归模型中使用$X$和$y$的数据结构如下，$X$包含多个样本，以及每个样本的属性，也就是自变量，和X的每个样本对应的就是我们的预测目标$y$，也就是因变量。\n",
    "\n",
    "在实际编程中，一般使用pd.DataFrame来表示$X$和$y$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d0bdd3-78cf-4c49-a9ab-a68b2ea591d0",
   "metadata": {},
   "source": [
    "![X&y](images/X&y.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97390d0f-4eca-4ae0-a2a5-2f7cb44eeabc",
   "metadata": {},
   "source": [
    "### 5.2 实现流程\n",
    "针对于多元回归分析，其一般化的流程如下：\n",
    "\n",
    "![regression_steps](images/regression_steps.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8994f9e-7fae-46ce-a0c4-ff710376c506",
   "metadata": {},
   "source": [
    "1）创建学习器，也就是初始化线性回归模型\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb44b1-503d-4143-a2c3-cfc2b12493e3",
   "metadata": {},
   "source": [
    "也可以这样表述：\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ef978-00a8-47a2-8040-70af7c062b4f",
   "metadata": {},
   "source": [
    "2）训练模型\n",
    "```python\n",
    "model.fit(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92793a74-c41f-42b9-8ac0-4f10eba90180",
   "metadata": {},
   "source": [
    "3）生成预测结果\n",
    "```python\n",
    "predicted_y = model.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c2db9-3f55-4f45-a699-d021843ccb73",
   "metadata": {},
   "source": [
    "4）计算模型预测精度和拟合优度$R^2$\n",
    "```python\n",
    "precision = model.score(X, y)\n",
    "```\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(predicted_y, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27646ec6-6c4e-4811-ba64-22b6a9a7284c",
   "metadata": {},
   "source": [
    "5）生成汇总信息（summary）\n",
    "```python\n",
    "print(\"系数: %s\" %model.coef_)\n",
    "print(\"截距: %.4f\" %model.intercept_)\n",
    "print(\"样本内(IS)训练集精度:%.2f\" %precision)\n",
    "print(\"拟合优度R-squared: %.2f\" % r2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a2a99-24ef-4a0e-a87a-9ec78689302e",
   "metadata": {},
   "source": [
    "## 参考\n",
    "1. https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-regression.html\n",
    "2. 詹姆斯\\*斯托克，马克\\*沃森《计量经济学》第三版\n",
    "1. sklearn官网：[链接](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
