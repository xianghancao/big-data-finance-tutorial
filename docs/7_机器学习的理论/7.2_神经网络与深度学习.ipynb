{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628a8119-58f2-41f6-a4b1-e10c330db3e9",
   "metadata": {},
   "source": [
    "# 7.2 神经网络与深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2501ee-cc0a-4f37-84f9-25bc0b6bc63b",
   "metadata": {},
   "source": [
    "## 深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03851b73-6f3e-4184-9583-02dda62fc2b8",
   "metadata": {},
   "source": [
    "深度学习是机器学习的一个特定子领域：一种从数据中学习表示的新方法，它强调学习越来越有意义的连续层次的表示。\n",
    "\n",
    "\"深度\"在\"深度学习\"中并不是指该方法实现了更深层次的理解；相反，它代表了逐渐学习连续层次的表示的想法。模型中有多少层贡献到数据的模型中被称为模型的深度。该领域的其他合适名称可能是分层表示学习或层次化表示学习。现代深度学习通常涉及到数十甚至数百个连续的表示层，它们都是通过暴露于训练数据而自动学习的。与此同时，其他机器学习方法往往只专注于学习数据的一两个表示层（比如，获取像素直方图然后应用分类规则）；因此，它们有时被称为浅层学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89ac44-c732-4d45-a4df-b7e60c7e4d5d",
   "metadata": {},
   "source": [
    "在深度学习中，这些分层表示是通过称为神经网络的模型学习的，这些模型被组织成逐层堆叠在一起的字面上的层次结构。术语\"神经网络\"是指神经生物学，但尽管深度学习的一些核心概念部分是受到我们对大脑的理解（尤其是视觉皮层）的启发而发展的，但深度学习模型并不是大脑的模型。没有证据表明大脑实现了任何类似于现代深度学习模型中使用的学习机制。你可能会遇到一些普及科学文章声称深度学习就像大脑一样工作或者是大脑的模型，但事实并非如此。让初学者误以为深度学习与神经生物学有任何关联会很令人困惑且适得其反；你不需要将深度学习包装成与我们的思维方式\"一模一样\"的神秘事物，你也可以忘记你可能读到的有关深度学习与生物学之间假想联系的任何东西。在我们这里，深度学习是一个从数据中学习表示的数学框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e23d2-10d5-4366-bb57-1daddbd2f9db",
   "metadata": {},
   "source": [
    "<img src='images/DL-digit-classification.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d6457-c18e-4eda-9b93-c92c7db10fc7",
   "metadata": {},
   "source": [
    "深度学习算法学习到的表示是什么样子的呢？让我们来看一看一个几层深的网络（见上图）是如何将一个数字的图像转换为识别它是什么数字的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab10a7-cf11-4a90-9ced-e5afd10d84aa",
   "metadata": {},
   "source": [
    "正如您在图下中所看到的，网络将数字图像转换为与原始图像越来越不同的表示，并且越来越能提供有关最终结果的信息。您可以将深度网络看作是一个多阶段的信息提取过程，在这个过程中，信息经过连续的过滤器，变得越来越纯净（也就是，对于某些任务而言更有用）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef58ce-a503-41fb-8682-358d1f43fce8",
   "metadata": {},
   "source": [
    "<img src='images/DL-digit-classification2.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fb61d6-1424-41f1-b424-cd22339e0c9a",
   "metadata": {},
   "source": [
    "因此，从技术上讲，这就是深度学习：一种学习数据表示的多阶段方法。这是一个简单的想法——但事实证明，足够大规模的简单机制最终可能看起来像魔术一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077e465-b6ab-4de8-8319-b4efef47ec33",
   "metadata": {},
   "source": [
    "## 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698fbedb-c808-421e-a3b3-1fa4fd7b49a6",
   "metadata": {},
   "source": [
    "神经网络(Neural Network，NN)既可以用于回归，也可以用于分类，但在实际应用中常用于分类。基于神经网络的深度学习因在图像识别和语音识别等领域表现优异而广为人知。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1c2881-6a1c-467b-aa46-ec770d8d6e76",
   "metadata": {},
   "source": [
    "<img src='images/NN.png' width=300>\n",
    "\n",
    "图 神经网络的典型网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee01ef-ca24-4b3b-971f-88798463ebab",
   "metadata": {},
   "source": [
    "神经网络在输入数据和输出结果之间插入了叫作中间层的层，能够学习复杂的决策边界。它既可以用于回归，也可以用于分类，但主要还是应用于分类问题。本 节也以分类问题为例进行说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bef383-e85d-47f1-91d9-bd017b9ce1b2",
   "metadata": {},
   "source": [
    "<img src='images/MNIST.png' width=300>\n",
    "\n",
    "图 MNIST 手写数字数据示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4e089-6e9c-4867-8949-cafc4ad48802",
   "metadata": {},
   "source": [
    "本次创建的神经网络如图所示，图中省略了各个节点之间的连接线。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c931ce4b-47af-48ab-9337-5acb87218857",
   "metadata": {},
   "source": [
    "<img src='images/MNIST-NN.png' width=600>\n",
    "\n",
    "图 MNIST神经网络构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb87905-321e-44d7-85d2-27bd92f20c3e",
   "metadata": {},
   "source": [
    "输入层表示输入图像(64 维向量)本身。如果将各个点的像素值存储在长度为 64 的一维数组的元 素中，则可以将其视为 64 维数据来处理。下面通过神经网络来学习使用上述方法得到的 64 维数据。\n",
    "中间层使用 Sigmoid 等非线性函数计算输入层传来的数据。中间层的维度是超参数。使维度变 大可以学习更加复杂的边界，但是容易发生过拟合。本次设置中间层为 16 维。下面的“算法说明” 部分将再次介绍中间层的计算方法、中间层的维度和学习结果的关系。\n",
    "输出层也同样使用非线性函数计算中间层传来的数据。本次任务是对 0 ~ 9 这 10 个数字进行 分类。因此，输出层输出的是输入手写图像分别为 0 ~ 9 这 10 个数字的概率。\n",
    "下面使用这个神经网络进行学习，并进行分类(图 4)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2effb7-8518-4fcb-ae5f-d457a70f589a",
   "metadata": {},
   "source": [
    "<img src='images/MNIST2.png' width=300>\n",
    "\n",
    "图 使用神经网络进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf5db63-4727-418b-9bbb-0dc9cd3ab58b",
   "metadata": {},
   "source": [
    "图中的 Label 代表作为正确答案的数字，Prediction 代表作为神经网络的分类结果的数字，下方的图像是输入数据。可以看出，神经网络可以正确地识别出这些手写数字。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b20944-5bb3-4195-93c8-bd93892d87b7",
   "metadata": {},
   "source": [
    "## 为什么是深度学习？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac19bc-4a23-47f3-925c-8e61a742e7f5",
   "metadata": {},
   "source": [
    "深度学习在计算机视觉领域的两个关键思想——卷积神经网络和反向传播——早在1990年就已经被充分理解。而对于时间序列的深度学习而言，长短期记忆（LSTM）算法于1997年开发出来，并且至今基本保持不变。那么，为什么深度学习直到2012年后才开始蓬勃发展？这两个十年间发生了什么变化呢？\n",
    "总的来说，机器学习的发展受到三个技术因素的推动：\n",
    "- 硬件\n",
    "- 数据集和基准\n",
    "- 算法进步\n",
    "由于这一领域的发展主要受到实验结果的指导，而非理论的支持，算法的进步只有在有适当的数据和硬件可用于尝试新思想（或扩展旧思想，这也是经常发生的情况）时才能实现。机器学习不像数学或物理学，可以用一支笔和一张纸完成重大进展。它是一门工程科学。\n",
    "在1990年代和2000年代，数据和硬件一直是真正的瓶颈。但是在此期间发生了以下变化：互联网迅速发展，高性能图形芯片应运而生，以满足游戏市场的需求。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68130c1e-08ba-452b-b43e-9d6f2f0f8be2",
   "metadata": {},
   "source": [
    "### 硬件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab9340-e74f-4256-9a1b-957e9c37e2ab",
   "metadata": {},
   "source": [
    "在1990年至2010年期间，现成的中央处理器（CPU）的速度提高了大约5000倍。因此，现在可以在笔记本电脑上运行小型深度学习模型，而在25年前这是不可行的。\n",
    "但是，在计算机视觉或语音识别中使用的典型深度学习模型需要比您的笔记本电脑提供的计算能力高出数个数量级。在2000年代，像NVIDIA和AMD这样的公司投资了数十亿美元开发快速、大规模并行的芯片（图形处理单元或GPU），以提供越来越逼真的视频游戏图形——这些廉价、单一用途的超级计算机旨在实时渲染屏幕上的复杂3D场景。当时，这项投资也使科学界受益匪浅。2007年，NVIDIA推出了CUDA（https://developer.nvidia.com/about-cuda） 编程接口。少数几个GPU开始取代各种高度可并行化的应用程序中的大型CPU集群，从物理建模开始。深度神经网络主要由许多小矩阵乘法组成，也高度可并行化，因此在2011年左右，一些研究人员开始编写神经网络的CUDA实现，Dan Ciresan和Alex Krizhevsky是其中的先驱之一。\n",
    "事实证明，游戏市场为下一代人工智能应用程序提供了超级计算的补贴。有时，大事物的开始就是游戏。如今，NVIDIA Titan RTX是一款GPU，在2019年底的价格为2500美元，可以提供16 teraFLOPS的单精度峰值性能（每秒16万亿个float32操作）。这比1990年世界上最快的超级计算机Intel Touchstone Delta强大约500倍。在Titan RTX上，仅需几个小时即可训练出类似于2012年或2013年ILSVRC竞赛获胜模型的ImageNet模型。与此同时，大型公司在数百个GPU的集群上训练深度学习模型。\n",
    "此外，深度学习行业已经超越了GPU，并正在投资于越来越专业、高效的深度学习芯片。在2016年的年度I/O大会上，谷歌公布了其张量处理单元（TPU）项目：一种新的芯片设计，从头开始开发，以比顶级GPU更快、更节能地运行深度神经网络。今天，2020年，TPU卡的第三个版本代表着420 teraFLOPS的计算能力。这比1990年的Intel Touchstone Delta强大约1万倍。\n",
    "这些TPU卡设计成可以组装成大规模的配置，称为“Pods”。一个Pod（1024个TPU卡）的峰值性能为100 petaFLOPS。以IBM Summit在奥克岭国家实验室的当前最大超级计算机的峰值计算能力为例，Summit由27,000个NVIDIA GPU组成，峰值计算能力约为1.1 exaFLOPS，这个Pod的峰值计算能力大约为其的10%。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caeba75-e398-44ed-bbae-5781129fd255",
   "metadata": {},
   "source": [
    "### 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7681c-3da3-453e-acf8-9d8343472681",
   "metadata": {},
   "source": [
    "人工智能有时被誉为新的工业革命。如果深度学习是这场革命的蒸汽机，那么数据就是它的煤炭：这种原材料为我们的智能机器提供动力，没有它就一无所能。在数据方面，除了过去20年存储硬件呈指数增长（遵循摩尔定律）之外，改变游戏规则的是互联网的崛起，使得收集和分发非常大型的机器学习数据集成为可能。如今，大型公司处理的图像数据集、视频数据集和自然语言数据集是通过互联网无法收集的。例如，Flickr上用户生成的图像标签一直是计算机视觉数据的宝库。YouTube视频也是如此。而维基百科是自然语言处理的关键数据集之一。\n",
    "如果说有一个数据集是深度学习崛起的催化剂，那就是ImageNet数据集，它包含了140万张图像，手工注释了1000个图像类别（每个图像一个类别）。但是ImageNet的特殊之处不仅仅在于它的规模之大，还有与之相关的年度竞赛。\n",
    "正如Kaggle自2010年以来一直在展示的那样，公开竞赛是激励研究人员和工程师突破极限的极好方式。研究人员竞争击败的共同基准有力地帮助了深度学习的崛起，突显了它相对于传统机器学习方法的成功。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed5745-3a41-4563-aab2-32356e175816",
   "metadata": {},
   "source": [
    "### 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9b285-a367-44c8-818c-89d1b6603e8b",
   "metadata": {},
   "source": [
    "除了硬件和数据之外，直到2000年代末，我们还缺乏一种可靠的方法来训练非常深的神经网络。因此，神经网络仍然相对较浅，只使用了一两层的表示；因此，它们无法与更精细的浅层方法（如支持向量机和随机森林）相提并论。关键问题在于梯度在深层堆叠中的传播。用于训练神经网络的反馈信号会随着层数的增加而逐渐消失。\n",
    "这在2009年至2010年左右发生了变化，当时出现了几项简单但重要的算法改进，使梯度传播更好：\n",
    "\n",
    "- 更好的神经网络层激活函数\n",
    "- 更好的权重初始化方案，从逐层预训练开始，然后很快就被抛弃了\n",
    "- 更好的优化方案，如RMSProp和Adam\n",
    "\n",
    "只有当这些改进开始允许训练具有10层或更多层的模型时，深度学习才开始发光。\n",
    "最后，在2014年、2015年和2016年，还发现了更先进的改进梯度传播的方法，如批归一化、残差连接和深度可分离卷积。\n",
    "如今，我们可以从头开始训练任意深的模型。这解锁了使用极其庞大的模型，这些模型具有相当丰富的表示能力，即编码非常丰富的假设空间。这种极端的可扩展性是现代深度学习的一个定义特征。大规模模型架构，具有数十层和数千万个参数，已经在计算机视觉（例如，ResNet、Inception或Xception等架构）和自然语言处理（例如，大型基于Transformer的架构，如BERT、GPT-3或XLNet）中带来了重要的进展。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88749e2-8db7-4cf1-8691-d9c871156053",
   "metadata": {},
   "source": [
    "## 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a55a06-77af-4b81-b65f-cc032ca6cd73",
   "metadata": {},
   "source": [
    "1. 秋庭伸也，《图解机器学习算法》\n",
    "2. Francois Chollet，Deep Learning with Python 2nd Edition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
